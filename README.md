# ğŸŒŸå››æŸ±æ¨å‘½ã¨å§“ååˆ¤æ–­ã‹ã‚‰äººç”Ÿã®ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆã‚’èª­ã¿è§£ãã‚¢ãƒ—ãƒª (MVP)

# æ¦‚è¦
å››æŸ±æ¨å‘½ã¨å§“ååˆ¤æ–­ã‹ã‚‰ã‚ãªãŸã®äººç”Ÿã®ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆã‚’ç‰©èªé¢¨ã«èª­ã¿è§£ãã‚¢ãƒ—ãƒªã§ã™ã€‚

ã€Œåå‰ã€ã€ã€Œç”Ÿã¾ã‚ŒãŸå¹´æœˆæ—¥æ™‚ã€ã‚’å…¥åŠ›ã™ã‚‹ã¨å‘½å¼ã€äº”è¡Œã€äº”æ ¼ãŒè¨ˆç®—ã•ã‚Œã¦å¤§ã¾ã‹ãªäººç”Ÿã®æµã‚ŒãŒæ¡ƒæºéƒ·ã®æ—…è·¯ã‚’æ¨¡ã—ãŸç‰©èªã§è¡¨ç¾ã•ã‚Œã¾ã™ã€‚

ç®—å‡ºã¯ä¸€èˆ¬çš„ãªã‚‚ã®ã‚’æ›´ã«ç°¡ç•¥åŒ–ã—ã¦ã„ã¾ã™ã€‚ç®—å‡ºã—ãŸå€¤ã‹ã‚‰LLMã§é‘‘å®šæ–‡ã‚’ä½œæˆã—ã¾ã™ã€‚

AIé§†å‹•é–‹ç™ºã®ç·´ç¿’ç”¨ã§ã™ã€‚
èªè¨¼ã€å³å¯†ãªãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒ­ã‚°ç›£è¦–ãªã©ã¯æœªå®Ÿè£…ã§ã™ã€‚


### ç”»é¢ã‚¤ãƒ¡ãƒ¼ã‚¸

<img src="images/preview1.png" height="400"><img src="images/preview2.png" height="400"><img src="images/preview4.png" height="400"><img src="images/preview5.png" height="400">

<img src="images/preview6.png" height="400"><img src="images/preview7.png" height="400">

# æ§‹æˆ
ã‚³ãƒ³ãƒ†ãƒŠã¯ä»¥ä¸‹ã®ã‚ˆã†ãªæ§‹æˆã§ã™
- frontend: node, react, next
- backend: python, fastapi
- worker: python, arq
- redis: redis
- db: poatgresql

```mermaid
flowchart TD
  %% ãƒãƒ¼ãƒ‰å®šç¾©
  user("`ãƒ¦ãƒ¼ã‚¶ãƒ¼
    (ãƒ–ãƒ©ã‚¦ã‚¶ / ã‚¹ãƒãƒ›)
  `")
  subgraph é–‹ç™ºå¯¾è±¡
    frontend["`frontendï¼ˆReact/Next.jsï¼‰
      ğŸ”µæ°åã€èª•ç”Ÿæ—¥ã€å‡ºç”Ÿæ™‚åˆ»ã®å…¥åŠ›
      ğŸ”µçµæœã®è¡¨ç¤º
      ğŸ”µé‘‘å®šå±¥æ­´ã®è¡¨ç¤º
    `"]
    backend["`backendï¼ˆFastAPIï¼‰
      ğŸ”µãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ï¼š
        å‘½å¼è¨ˆç®—ã€äº”è¡Œã€äº”æ ¼
      ğŸ”µLiteLLMç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
      ğŸ”µé‘‘å®šçµæœç”Ÿæˆ
    `"]
    worker["`workerï¼ˆarqï¼‰
      ğŸ”µã‚¸ãƒ§ãƒ–ã‚­ãƒ¥ãƒ¼ç®¡ç†
    `"]
    redis["`redis
      ğŸ”µã‚¸ãƒ§ãƒ–æƒ…å ±ä¿å­˜
    `"]
    db["`dbï¼ˆpostgresqlï¼‰
      ğŸ”µé‘‘å®šå±¥æ­´ä¿å­˜
    `"]
  end
  openai("OpenAI API")
  google("Google Gemini API")
  monitor("`[æœªå®Ÿè£…] ç›£è¦–ãƒ»ãƒ­ã‚°
    APIãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“/ã‚¨ãƒ©ãƒ¼ç‡
    ãƒ¢ãƒ‡ãƒ«åˆ¥ã‚³ã‚¹ãƒˆï¼ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡
    ï¼ˆGrafana/Loki/CloudWatchç­‰ï¼‰
  `")

  user <-->|èªè¨¼ã¯å¾Œå›ã—| frontend
  frontend <--> backend
  backend <--> worker
  worker <--> redis
  backend <--> db
  backend --> monitor
  backend <--> openai
  backend <--> google

  %% å¹…ã‚’æŒ‡å®šã™ã‚‹ã‚¯ãƒ©ã‚¹å®šç¾©ï¼ˆpxã§æŒ‡å®šï¼‰

  classDef wideCard stroke:#333,stroke-width:1px
  class * wideCard
  classDef dev fill:#fff,stroke:#333,stroke-width:2px,width:400px;text-align:center
  class frontend,backend,worker,redis,db dev

```


## è¨­å®šæ–¹æ³•

### APIã‚­ãƒ¼ è¨­å®š
1. [Google AI Studio](https://aistudio.google.com/)ã§API keyã‚’å–å¾—ã—ã¦ãã ã•ã„ã€‚
2. REPOãƒ«ãƒ¼ãƒˆã®`.env.sample`ã‚’`.env`ã«ãƒ•ã‚¡ã‚¤ãƒ«åå¤‰æ›´
3. `GEMINI_API_KEY=`ã« `1.` ã§å–å¾—ã—ãŸã‚­ãƒ¼ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦è²¼ã‚Šä»˜ã‘ã¾ã™

### ã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•ã€ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
```bash
# from repo root
docker compose up --build -d

# DBã®ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
# "$DATABASE_URL"ã¯ã”è‡ªèº«ã®ç’°å¢ƒã«åˆã‚ã›ã¦ä¿®æ­£ã—ã¦ãã ã•ã„
DATABASE_URL="postgresql://postgres:password@localhost:5432/fortunes"
psql -v -d "$DATABASE_URL" -f backend/migrations/init.sql

# æ¼¢å­—ãƒ‡ãƒ¼ã‚¿
BACKUP_PATH="backend/migrations/kanji.dump"
pg_restore -v -d "$DATABASE_URL" --clean --no-owner --no-privileges "$BACKUP_PATH"
```
PostgreSQLã®localeï¼šja_JP.UTF-8ã€futuresãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã® collationã‚‚'ja_JP.UTF-8'ã§æŒ‡å®šã€‚
èª•ç”Ÿæ—¥æ™‚ã¯å†…éƒ¨ã§UTCã¨ã—ã¦ä¿å­˜ã—ã€æŒ‡å®šã•ã‚ŒãŸtimezoneã«æˆ»ã—ã¦è¿”ã—ã¦ã„ã¾ã™ã€‚


### ãƒ–ãƒ©ã‚¦ã‚¶ã‚¢ã‚¯ã‚»ã‚¹

`http://localhost:3000`
ã§ç”»é¢ãŒè¡¨ç¤ºã•ã‚Œã¾ã™

### LLMã®ãƒ¢ãƒ‡ãƒ«

é‘‘å®šæ–‡ä½œæˆã¨ã‚µãƒãƒªä½œæˆã§ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’åˆ†ã‘ã¦ã„ã¾ã™ã€‚
- é‘‘å®šæ–‡ä½œæˆï¼š gemini-2.5-flash
- ã‚µãƒãƒªä½œæˆï¼š gemini-2.5-flash-lite

é‘‘å®šæ–‡ã¯è¡¨ç¾åŠ›ãŒå¿…è¦ãªã®ã§ã€`gemini-2.5-pro`ã‚„OpenAIã®`GPT-4o`ã®ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ãŒãŠã™ã™ã‚ã§ã™ã€‚ã“ã®ç’°å¢ƒã§ã¯ç„¡æ–™æ ãŒã‚ã‚‹gemini-2.5-flashã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
ã‚µãƒãƒªä½œæˆã¯è¡¨ç¾åŠ›ãŒå•ã‚ã‚Œãªã„ã®ã§ã€`gemini-2.5-flash-lite`ã®ã‚ˆã†ãªè»½ãã¦å®‰ä¾¡ãªãƒ¢ãƒ‡ãƒ«ãŒã‚ˆã„ã§ã™ã€‚

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚‚[é‘‘å®šæ–‡](backend/app/services/prompts/template_life_analysis.py)ã¨[ã‚µãƒãƒª](backend/app/services/prompts/template_life_analysis_summary.py)ã§åˆ†ã‘ã¦ã„ã¾ã™


### é–‹ç™ºç’°å¢ƒç”¨ãƒ„ãƒ¼ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
- ãƒªãƒ³ã‚¿ãƒ¼ã€ã‚³ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™

```
pip install -r dev-requirements.txt
```

### TEST
ãƒ†ã‚¹ãƒˆã‚³ãƒãƒ³ãƒ‰

```bash
docker compose exec frontend npm test -- --coverage --coverageDirectory=coverage --coverageReporters=text
docker compose exec backend bash -c "PYTHONPATH=/app pytest"
```

### debug
uvicornã¨sqlalchemyã®debugã‚’æœ‰åŠ¹åŒ–
```bash
docker compose -f docker-compose.yml -f docker-compose.override.yml up --build
```




## æ°¸ç¶šé‹ç”¨
<!-- ### é‹ç”¨(systemd)ï¼š -->
 <!-- Service Unit (æ¨å¥¨): docker compose ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã‚’ systemd ã§ç®¡ç†ã™ã‚‹ã®ãŒç°¡å˜ã§å …ç‰¢ -->
<!-- ```
[Unit]
Description=Fortunes Docker Compose
After=docker.service
Requires=docker.service

[Service]
Type=oneshot
RemainAfterExit=yes
WorkingDirectory=/home/agake/work/fortunes
ExecStart=/usr/bin/docker compose up -d
ExecStop=/usr/bin/docker compose down
TimeoutStartSec=0

[Install]
WantedBy=multi-user.target
``` -->
<!-- å€‹åˆ¥ã‚³ãƒ³ãƒ†ãƒŠç®¡ç†: ã‚‚ã— worker ã®ã¿æ°¸ç¶šåŒ–ã—ãŸã‘ã‚Œã° ExecStart=/usr/bin/docker compose up -d worker ã‚’ä½¿ã†ã€‚
æ³¨æ„: ãƒ¦ãƒ¼ã‚¶ãƒ¼å˜ä½ã§å‹•ã‹ã™å ´åˆã¯ user-level systemd ã‚‚å¯ã€‚systemd ã® Restart= ãƒãƒªã‚·ãƒ¼ã‚„ StartLimitBurst ã§å†èµ·å‹•åˆ¶å¾¡ã€‚ -->

### ãƒ­ã‚°
- ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
  - `docker-compose.yml`ã®`logging:`ã§è¨­å®š
<!-- é›†ä¸­ãƒ­ã‚°åŸºç›¤: Loki/Promtail, ELK (Elasticsearch/Logstash/Kibana), Fluentd/Graylog ãªã©ã¸é€ã‚‹ã®ãŒæ¨å¥¨ï¼ˆæ¤œç´¢ã¨ã‚¢ãƒ©ãƒ¼ãƒˆãŒå®¹æ˜“ï¼‰ã€‚
ã‚¨ãƒ©ãƒ¼ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°: Sentry ã‚’å°å…¥ã—ã¦ä¾‹å¤–ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’åé›†ã€‚Worker/Server ã« SDK ã‚’å…¥ã‚Œã‚‹ã ã‘ã§OKã€‚
ãƒ­ã‚°ä¿æŒæ–¹é‡: æ³•ä»¤ã‚„å®¹é‡ã«åˆã‚ã›ã¦ä¿ç®¡æœŸé–“ã‚’æ±ºã‚ã€å¤ã„ãƒ­ã‚°ã¯åœ§ç¸®/å‰Šé™¤ã€‚ -->

<!-- ### ç›£è¦–ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆï¼‰: -->

<!-- ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†: Prometheus + Grafanaã€‚ã‚¢ãƒ—ãƒªå´ã« /metrics ã‚’å…¬é–‹ï¼ˆFastAPI ã« prometheus_client ã¾ãŸã¯ fastapi-prometheus ã‚’å°å…¥ï¼‰ã€‚
é‡è¦ãªæŒ‡æ¨™:
job queue lengthï¼ˆRedis ã®å¾…ã¡è¡Œåˆ—é•·ï¼‰
job failure rate / error countï¼ˆWorkerï¼‰
job processing timeï¼ˆé…å»¶æ¤œå‡ºï¼‰
Redisãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã€Postgresæ¥ç¶šæ•°ã€CPU/ãƒ¡ãƒ¢ãƒªã‚³ãƒ³ãƒ†ãƒŠãƒªã‚½ãƒ¼ã‚¹
ã‚µãƒ³ãƒ—ãƒ«ã‚¢ãƒ©ãƒ¼ãƒˆï¼ˆPromQLï¼‰:
jobå¤±æ•—ç‡ > 1% (5m)
Redis memory > 80%
jobå‡¦ç†æ™‚é–“ã®95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ« > 30s
å¯è¦–åŒ–: Grafana ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§SLAæŒ‡æ¨™ã¨æœ€è¿‘ã®å¤±æ•—ã‚’è¡¨ç¤ºã€‚ -->

### ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ / Liveness & Readiness:
 FastAPI ã« /health ï¼ˆLivenessï¼‰ã¨ /ready ï¼ˆReadiness ã€DBãƒ»Redisæ¥ç¶šãƒã‚§ãƒƒã‚¯ï¼‰ã‚’è¿½åŠ ã€‚Kubernetes ã§ã®é‹ç”¨ã‚„ systemd å´ã®ç›£è¦–ã§ä½¿ã†ã€‚


<!-- ### ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚° & åˆ†æ: -->

<!-- åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°: OpenTelemetry + Jaegerï¼ˆLLMå‘¼ã³å‡ºã—ã‚„ DB ã‚¯ã‚¨ãƒªã®é…å»¶èª¿æŸ»ã«æœ‰åŠ¹ï¼‰ã€‚
ã‚µãƒ³ãƒ—ãƒ«: opentelemetry-instrumentation-fastapi ã‚’å°å…¥ã—ã¦è‡ªå‹•è¨ˆæ¸¬ã€‚ -->

### æ°¸ç¶šãƒ‡ãƒ¼ã‚¿ç®¡ç† / ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—:
```bash
# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— backupãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«fortunes-ooooooooooooooo.dumpã§ä¿å­˜
scripts/backup_fortunes.sh

# ãƒ¬ã‚¹ãƒˆã‚¢
scripts/restore_fortunes.sh backup/fortunes-ooooooooooooooo.dump
```
<!-- Postgres ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: å®šæœŸçš„ãª pg_dump / WAL ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã€‚è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ + S3 ãªã©ã¸ã®ä¿å­˜ã€‚
DB ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç®¡ç†: alembic ç­‰ã§ã‚¹ã‚­ãƒ¼ãƒç®¡ç†ã¨ãƒªãƒªãƒ¼ã‚¹æ‰‹é †ã‚’ç¢ºç«‹ã€‚ -->

<!-- ### é‹ç”¨ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆé€šçŸ¥ãƒ»Runbookï¼‰: -->

<!-- ã‚¢ãƒ©ãƒ¼ãƒˆé€šçŸ¥: Slack/Email/PagerDuty ã¸é€šçŸ¥ï¼ˆGrafana/Alertmanager çµŒç”±ï¼‰ã€‚
Runbook: ä»£è¡¨çš„å•é¡Œï¼ˆRedisæ¥ç¶šåˆ‡æ–­ã€LLM APIã‚­ãƒ¼åˆ‡ã‚Œã€DBæ¥ç¶šæ¯æ¸‡ï¼‰ã®å¾©æ—§æ‰‹é †ã‚’æ–‡æ›¸åŒ–ã€‚
ç›£æŸ»ãƒ­ã‚°: ä¸»è¦æ“ä½œï¼ˆè¨­å®šå¤‰æ›´ãƒ»deployãƒ»DB restoreï¼‰ã®è¨˜éŒ²ã€‚ -->

<!-- ### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ»è¨­å®šç®¡ç†: -->

<!-- ç§˜å¯†ç®¡ç†: ç’°å¢ƒå¤‰æ•°ã‚’ç›´æ¥ç½®ã‹ãš Vault / AWS Secrets Manager ç­‰ã§ç®¡ç†ã€‚
ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡: DB/Redis ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ã‚¯ã‚»ã‚¹ã¯å†…éƒ¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«é™å®šã€‚ -->



## ãã®ä»–
### (å‚è€ƒ)ã‚¸ãƒ§ãƒ–ã®ã‚­ãƒ¥ãƒ¼ç®¡ç† Arq backendã‚³ãƒ³ãƒ†ãƒŠã§ç¢ºèªã™ã‚‹
```bash
$ docker compose exec backend bash -lc "PYTHONPATH=/app  python -m app.worker"

07:57:07: Starting worker for 1 functions: app.tasks.process_analysis
07:57:07: redis_version=7.4.7 mem_usage=1.01M clients_connected=1 db_keys=0


$ curl -X POST http://localhost:8000/analyze/enqueue \
  -H "Content-Type: application/json" \
  -d '{"name_sei":"å¤ª","name_mei":"éƒ","birth_date":"1990-01-01","birth_hour":12, "birth_tz":"Asia/Tokyo"}'
```




## æ¼¢å­—ã®ç”»æ•°DBã«ã¤ã„ã¦
æ¼¢å­—ã®ç”»æ•°ã¯[æ¼¢å­—ç”»æ•°ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹](https://kanji-database.sourceforge.net/database/strokes.html)ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã›ã¦ã„ãŸã ãã¾ã—ãŸã€‚

- ãƒ•ã‚¡ã‚¤ãƒ«ï¼šbackend/migrations/ucs-strokes.txt,v
- æ¼¢å­—ç”»æ•°ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–¹æ³•
  `PYTHONPATH=./backend python backend/import_kanji.py`

